{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictorStarter import * # this file contains the import of every dataset, libraries needed and the initial plotting of the data\n",
    "from predictorsImplementation import * # this file contains the implementation of the predictors ( one function implementation for each predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for plotting and compare inside a unique chart prices and volatilities for mgarch predictions\n",
    "\n",
    "def plot_prices_volatilities_for_mgarch(stock_prices, real_volatility, real_volatility_startDate, real_volatility_endDate, mgarchVolatility, asset_name):\n",
    "    '''\n",
    "    Function to plot prices and volatilities for MGARCH\n",
    "    '''\n",
    "     # filter the real volatility between the start and end date\n",
    "    real_volatility_startDate = pd.to_datetime(real_volatility_startDate)\n",
    "    real_volatility_endDate = pd.to_datetime(real_volatility_endDate)\n",
    "\n",
    "    # Correct way to filter using & operator and parentheses\n",
    "    real_volatility_filtered = real_volatility[(real_volatility.index >= real_volatility_startDate) & (real_volatility.index <= real_volatility_endDate)]\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 11), sharex=True)\n",
    "\n",
    "    # Plot stock prices\n",
    "    ax1.plot(stock_prices[asset_name], label=f'{asset_name} Price', color='green')\n",
    "    ax1.set_title(f'{asset_name} Stock Prices')\n",
    "    ax1.set_ylabel('Price(dollars)')\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Plot real and rolling window volatilities\n",
    "    ax2.plot(real_volatility_filtered, label=f'Real {asset_name} Volatility', color='blue')\n",
    "    ax2.plot(mgarchVolatility, label=f'Mgarch {asset_name} Volatility', color='orange', linestyle='--')\n",
    "    ax2.set_title(f'{asset_name} Volatility: Real vs MGARCH')\n",
    "    ax2.set_xlabel('Time(days)')\n",
    "    ax2.set_ylabel('Volatility(%)')\n",
    "    ax2.legend(loc='upper left')\n",
    "\n",
    "    # Set x-axis limits to match the start and end dates\n",
    "    ax1.set_xlim(left=real_volatility_startDate, right=real_volatility_endDate)\n",
    "    ax2.set_xlim(left=real_volatility_startDate, right=real_volatility_endDate)\n",
    "\n",
    "    # Adding vertical lines for specific events\n",
    "    ax1.axvline(pd.Timestamp('2020-02-24'), color='gray', linestyle='--', lw=2)  # COVID start\n",
    "    ax1.axvline(pd.Timestamp('2022-02-24'), color='red', linestyle='--', lw=2)  # Ukraine War start\n",
    "    \n",
    "    # Adding vertical lines for specific events\n",
    "    ax2.axvline(pd.Timestamp('2020-02-24'), color='gray', linestyle='--', lw=2)  # COVID start\n",
    "    ax2.axvline(pd.Timestamp('2022-02-24'), color='red', linestyle='--', lw=2)  # Ukraine War start\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prescientDict = {}\n",
    "daily_log_likelihoods = {}\n",
    "daily_regrets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRESCIENT(GROUND TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE IS CALCULATING THE REAL VOLATILITY\n",
    "\n",
    "# prescient is a dictionary that contains the covariance matrix calculated using the ewma formula written inside the paper\n",
    "# the key of the dictionary is the timestamp and the value is the covariance matrix calculated for that day\n",
    "\n",
    "# The prescient predictor will always use the original dataset, so it will be uniformly distributed; this is because the prescient predictor is used to compare the other predictors\n",
    "# and we need to have a measure of the real covariance matrix; so this can't be used with the non-uniformly distributed dataset\n",
    "\n",
    "prescientDict = originalPrescientPredictor(uniformlyDistributedReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE VISUALIZATION OF THE REAL VOLAITILITIES OF THE 3 ASSETS\n",
    "\n",
    "# now calculates/extract the real volatilities of the 3 assets\n",
    "real_volatilities = {}\n",
    "\n",
    "for date, cov_matrix in prescientDict.items():\n",
    "    volatilities = np.sqrt(np.diag(cov_matrix.values))\n",
    "    real_volatilities[date] = pd.DataFrame(data = volatilities, index = cov_matrix.index, columns = [\"volatility\"])\n",
    "\n",
    "# now real_volatilities is a dictionary that contains the real volatilities of the 3 assets for every day with the same key of the prescientDict dictionary(the timestamp)\n",
    "\n",
    "# now separate the real volatilities of the 3 assets in 3 different dataframes\n",
    "volatility_dict_aapl = {}\n",
    "volatility_dict_ibm = {}\n",
    "volatility_dict_mcd = {}\n",
    "\n",
    "for date, volatilities in real_volatilities.items():\n",
    "    volatility_dict_aapl[date] = volatilities.loc[7][\"volatility\"] # 7 is the PERMCO code of AAPL\n",
    "    volatility_dict_ibm[date] = volatilities.loc[20990][\"volatility\"] # 20990 is the PERMCO code of IBM\n",
    "    volatility_dict_mcd[date] = volatilities.loc[21177][\"volatility\"] # 21177 is the PERMCO code of MCD\n",
    "\n",
    "\n",
    "# Convert the dictionaries to DataFrames for easier manipulation and plotting\n",
    "df_volatility_aapl = pd.DataFrame(list(volatility_dict_aapl.items()), columns=['Date', 'AAPL Volatility'])\n",
    "df_volatility_ibm = pd.DataFrame(list(volatility_dict_ibm.items()), columns=['Date', 'IBM Volatility'])\n",
    "df_volatility_mcd = pd.DataFrame(list(volatility_dict_mcd.items()), columns=['Date', 'MCD Volatility'])\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "df_volatility_aapl.set_index('Date', inplace=True)\n",
    "df_volatility_ibm.set_index('Date', inplace=True)\n",
    "df_volatility_mcd.set_index('Date', inplace=True)\n",
    "\n",
    "# Plot the real volatilities of the 3 assets\n",
    "plt.figure(figsize=(18, 11))\n",
    "plt.plot(df_volatility_aapl, label='AAPL Volatility')\n",
    "plt.plot(df_volatility_ibm, label='IBM Volatility')\n",
    "plt.plot(df_volatility_mcd, label='MCD Volatility')\n",
    "plt.legend()\n",
    "plt.title(\"Real Volatilities of the 3 assets\")\n",
    "plt.xlabel(\"Time(days)\")\n",
    "plt.ylabel(\"Volatility(%)\")\n",
    "\n",
    "# Adding vertical lines for specific events\n",
    "plt.axvline(pd.Timestamp('2020-02-24'), color='gray', linestyle='--', lw=2)  # COVID start\n",
    "plt.axvline(pd.Timestamp('2022-02-24'), color='orange', linestyle='--', lw=2)  # Ukraine War start\n",
    "\n",
    "# Annotations for the events\n",
    "plt.text(pd.Timestamp('2020-02-24'), plt.ylim()[1], 'COVID', horizontalalignment='center', color='gray')\n",
    "plt.text(pd.Timestamp('2022-02-24'), plt.ylim()[1], 'Ukraine War', horizontalalignment='center', color='orange')\n",
    "\n",
    "# Set x-axis limits to the first and last index of your time series data\n",
    "plt.xlim(df_volatility_aapl.index[0], df_volatility_aapl.index[-1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR THE MGARCH PREDICTOR I IMPORT THE COVARIANCE MATRICES ALREDY CALCULATED; SO IT'S LIKE THE TRAINING PART HAS BEEN DONE ALREADY, AND I NEED TO VERY THAT THE PREDICTOR IS WORKING PROPERLY(SO I NEED TO VERIFY THE VALIDATION PART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Phase for MGARCH Predictor\n",
    "\n",
    "with the armaOrder = (10,10) and the garchOrder = (10,10) and dccorder = (10,10) the performance are worse respect the case with armaOrder = (1,1) and garchOrder = (1,1) and dccorder = (1,1) -> at the moment the best performance are with the smallest order(this because the performance are more or less equal but the number of parameters are smaller in the case with the smallest order) -> with the smallest order the number of parameters are 17; the number of parameters with the order (10,10) are 149. so i prefer the smallest order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mgarchAnalysis.flattenCovMatrices import *\n",
    "\n",
    "def callRprocessAndUpdateCSVfile(csv_file_path, r_file_path):\n",
    "    '''\n",
    "    Function to call the R process and update the csv validation file\n",
    "    '''\n",
    "\n",
    "    # modify the environment variable USE_HYBRID_MGARCH to false\n",
    "    os.environ['USE_HYBRID_MGARCH'] = 'False'\n",
    "\n",
    "    csv_file_path = file_path_first_part + csv_file_path\n",
    "    # Define the path to your R script\n",
    "    r_script_path = file_path_first_part + r_file_path \n",
    "\n",
    "    # Define the command to run your R script\n",
    "    command = ['Rscript', r_script_path]\n",
    "\n",
    "    returnCode = 1\n",
    "\n",
    "    while returnCode != 0:\n",
    "        \n",
    "        # Execute the command\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        # if the code fails there is a problem of convergence so i keep trying to run the R script until it converges\n",
    "        # Check if there was an error and raise an exception if so\n",
    "        if result.returncode != 0:\n",
    "            print(\"Error running the R script:\")\n",
    "            print(result.stderr)\n",
    "        else:\n",
    "            # If no error, print the output\n",
    "            print(\"R script executed successfully:\")\n",
    "            print(result.stdout)\n",
    "            returnCode = 0\n",
    "\n",
    "    # Update the csv file with the validation results\n",
    "    flattenAllMatrices(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MGARCH precomputed in R due to computational complexity\n",
    "# now to automatize the process of obtaining the MGARCH predictions, i will call a function that will execute the r script that calculates the MGARCH predictions\n",
    "# so the csv is updated with the MGARCH predictions and then i will read the csv file to obtain the MGARCH predictions\n",
    "\n",
    "callRprocessAndUpdateCSVfile(\"AllCovMatricesForValidation.csv\", 'experiments/mgarchAnalysis/dccGarchTraningValidationTuning.r')\n",
    "\n",
    "# The covariance matrices were computed for 100*r for numerical reasons \n",
    "\n",
    "mgarch_cond_cov = pd.read_csv(\"mgarch_predictors_from_R/stocks/FlattenedAllCovMatricesForValidation.csv\", index_col=None)\n",
    "\n",
    "# here we are obtaining the covariance matrix calculated for every day from the csv file;\n",
    "# so this covariance matrix estimation is done with real returns and not with interpolated returns\n",
    "Sigmas = from_row_matrix_to_covariance(mgarch_cond_cov.values, stocksPercentageChangeReturn.shape[1]) / 10000 # returns.shape[1] gives the number of columns in the returns DataFrame, which corresponds to the number of assets in the portfolio\n",
    "\n",
    "print(\"sigma shape:\", Sigmas.shape)\n",
    "# Remove the training dataset \n",
    "trainingSetLength = len(trainingDataWithPercentageChange)\n",
    "testSetLength = len(testDataWithPercentageChange)\n",
    "times = stocksPercentageChangeReturn.loc[validation_start_date:validation_end_date].index\n",
    "\n",
    "print(\"len of times: \", len(times))\n",
    "\n",
    "mgarchDict = {times[t]: pd.DataFrame(Sigmas[t], index=stocksPercentageChangeReturn.columns, columns=stocksPercentageChangeReturn.columns) for t in range(Sigmas.shape[0])}\n",
    "\n",
    "print(\"mgarchDict len:\", len(mgarchDict))\n",
    "# print just the first key and value of the dictionary\n",
    "print(list(mgarchDict.keys())[0]) # TODO: delete this print\n",
    "print(mgarchDict[list(mgarchDict.keys())[0]]) # TODO: delete this print\n",
    "print(\"\\n\")\n",
    "print(list(mgarchDict.keys())[-1]) # TODO: delete this print\n",
    "print(mgarchDict[list(mgarchDict.keys())[-1]]) # TODO: delete this print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now see how well the mgarch predictor is doing in predicting the real volatilities of the 3 assets by calculating the log-likelihood and the regret and mse\n",
    "\n",
    "# Define start and end of backtest; first two years used for training/burn-in\n",
    "startingValidationDate = validationDataWithPercentageChange.index[0].strftime(\"%Y-%m-%d\")\n",
    "endingValidationDate = validationDataWithPercentageChange.index[-1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "start_date = pd.to_datetime(startingValidationDate, format=\"%Y-%m-%d\")\n",
    "end_date = pd.to_datetime(endingValidationDate, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"MGARCH\", \"PRESCIENT\"]\n",
    "\n",
    "#these predictors are all dictionaries where each entry contains a Pandas DataFrame representing a covariance matrix of returns at each timestamp.  \n",
    "predictors_temp = [mgarchDict, prescientDict]\n",
    "predictors = [] # so this is a list of dictionaries\n",
    "\n",
    "for predictor in predictors_temp:\n",
    "    predictors.append({t: predictor[t] for t in predictor.keys() if t >= start_date and t <= end_date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics to evaluate during the validation period: log-likelihood, regret, and MSE\n",
    "\n",
    "'''\n",
    "    this dictionary has a shape like this:\n",
    "    {\n",
    "        RW: pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times),\n",
    "        EWMA: pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times),\n",
    "        MGARCH: pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times),\n",
    "        PRESCIENT: pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times),\n",
    "    }\n",
    "\n",
    "    where each pd.series is a series of log-likelihoods for each timestamp: so there is the log-likelihood value for each timestamp\n",
    "'''\n",
    "\n",
    "log_likelihoods = {}\n",
    "for i, predictorDict in enumerate(predictors):\n",
    "\n",
    "    # if the predictor is the prescient predictor, i have to use the uniformly distributed dataset\n",
    "    if names[i] == \"PRESCIENT\":\n",
    "        returns_temp = uniformlyDistributedReturns.loc[pd.Series(predictorDict).index].values[1:]\n",
    "    \n",
    "    else:\n",
    "        returns_temp = stocksPercentageChangeReturn.loc[pd.Series(predictorDict).index].values[1:]\n",
    "\n",
    "    times = pd.Series(predictorDict).index[1:]\n",
    "    Sigmas_temp = np.stack([predictorDict[t].values for t in predictorDict.keys()])[:-1]       \n",
    "    daily_log_likelihoods[names[i]] = pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times)\n",
    "\n",
    "\n",
    "# Iterate through each predictor in the log_likelihoods dictionary\n",
    "for name in daily_log_likelihoods.keys():\n",
    "    if name == 'PRESCIENT':\n",
    "        # Resample by quarter, take the mean, and plot with specific color and label\n",
    "        daily_log_likelihoods[name].resample(\"Q\").mean().plot(label=name, c=\"k\")\n",
    "    else:\n",
    "        # Resample by quarter, take the mean, and plot with default settings\n",
    "        daily_log_likelihoods[name].resample(\"Q\").mean().plot(label=name)\n",
    "\n",
    "plt.xlabel('Time(quarter)')  # Set the x-axis label\n",
    "plt.ylabel('Log Likelihood')  # Set the y-axis label\n",
    "plt.title('Quarterly Mean Log Likelihood by Predictor')  # Set the title of the plot\n",
    "plt.legend()  # Show the legend to identify each predictor\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "\n",
    "'''\n",
    "    this dictionary has a shape like this:\n",
    "    {\n",
    "        RW: pd.Series(...),\n",
    "        EWMA: pd.Series(...),\n",
    "        MGARCH: pd.Series(...),\n",
    "        PRESCIENT: pd.Series(...),\n",
    "    }\n",
    "\n",
    "    where each pd.series is a series of regret for each timestamp: so there is the \n",
    "    regret value (the difference between the log-likelihood of the prescient model and the log-likelihood of the model) for each timestamp\n",
    "'''\n",
    "for name in daily_log_likelihoods:\n",
    "    daily_regrets[name] =  daily_log_likelihoods[\"PRESCIENT\"] - daily_log_likelihoods[name]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "for name in names:\n",
    "    if name == 'PRESCIENT':\n",
    "        pass\n",
    "    else:\n",
    "        daily_regrets[name].resample(\"Q\").mean().plot(label=name)\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.1), loc='center', ncols=4, labels=names[:-1], scatterpoints=1, markerscale=5);\n",
    "plt.xlabel(\"Time(quarter)\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(\"Regret\")\n",
    "\n",
    "\n",
    "for name in daily_regrets:\n",
    "    if name != \"PRESCIENT\":\n",
    "\n",
    "        #Each data point in the regret series now represents the average regret for a respective quarter. If the original series spans multiple years, then the number of data points in regret will be the number of quarters in that time frame.\n",
    "        quarterly_regrets = daily_regrets[name].resample(\"Q\").mean() #it resamples the regret Series to a quarterly frequency, This gives the average regret for each quarter rather than daily regret values  \n",
    "        # so the regret variable is a series of average regret for each quarter\n",
    "        \n",
    "        regretMetrics = (np.mean(quarterly_regrets).round(1), np.std(quarterly_regrets).round(1), np.max(quarterly_regrets).round(1))\n",
    "        # the round(1) function to each of these metrics, which rounds the result to one decimal place,\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"meanRegret: {regretMetrics[0]:.3f}\")\n",
    "print(f\"stdRegret: {regretMetrics[1]:.3f}\")\n",
    "print(f\"maxRegret: {regretMetrics[2]:.3f}\")\n",
    "\n",
    "# copy the log-likelihoods dictionary\n",
    "daily_log_likelihoods_copy = daily_log_likelihoods.copy()\n",
    "\n",
    "# do the same thing for log-likelihoods dictionary\n",
    "for name in daily_log_likelihoods_copy:\n",
    "    quarterly_logLikelihood = daily_log_likelihoods_copy[name].resample(\"Q\").mean()\n",
    "    logLikelihoodMetrics = (np.mean(quarterly_logLikelihood).round(1), np.std(quarterly_logLikelihood).round(1), np.max(quarterly_logLikelihood).round(1))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"meanLoglikelihood{name}: {logLikelihoodMetrics[0]:.3f}\")\n",
    "    print(f\"stdLoglikelihood{name}: {logLikelihoodMetrics[1]:.3f}\")\n",
    "    print(f\"maxLoglikelihood{name}: {logLikelihoodMetrics[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the date for the test phase\n",
    "startingValidationDate = validationDataWithPrices.index[0].strftime(\"%Y-%m-%d\")\n",
    "endingValidationDate = validationDataWithPrices.index[-1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# read from this file the covariance matrices calculated for the test phase\n",
    "mgarch_cond_cov = pd.read_csv(\"mgarch_predictors_from_R/stocks/FlattenedAllCovMatricesForValidation.csv\", index_col=None)\n",
    "\n",
    "# here we are obtaining the covariance matrix calculated for every day from the csv file;\n",
    "# so this covariance matrix estimation is done with real returns and not with interpolated returns\n",
    "Sigmas = from_row_matrix_to_covariance(mgarch_cond_cov.values, stocksPercentageChangeReturn.shape[1]) / 10000 # returns.shape[1] gives the number of columns in the returns DataFrame, which corresponds to the number of assets in the portfolio\n",
    "\n",
    "print(\"sigma shape:\", Sigmas.shape)\n",
    "# Remove the training dataset \n",
    "trainingSetLength = len(trainingDataWithPercentageChange)\n",
    "testSetLength = len(testDataWithPercentageChange)\n",
    "times = stocksPercentageChangeReturn.loc[validation_start_date:validation_end_date].index\n",
    "\n",
    "print(\"len of times: \", len(times))\n",
    "\n",
    "mgarchDict = {times[t]: pd.DataFrame(Sigmas[t], index=stocksPercentageChangeReturn.columns, columns=stocksPercentageChangeReturn.columns) for t in range(Sigmas.shape[0])}\n",
    "\n",
    "print(\"mgarchDict len:\", len(mgarchDict))\n",
    "# calculate the covariance matrix for the test phase\n",
    "\n",
    "# print just the first key and value of the dictionary\n",
    "print(list(mgarchDict.keys())[0]) # TODO: delete this print\n",
    "print(mgarchDict[list(mgarchDict.keys())[0]]) # TODO: delete this print\n",
    "print(\"\\n\")\n",
    "print(list(mgarchDict.keys())[-1]) # TODO: delete this print\n",
    "print(mgarchDict[list(mgarchDict.keys())[-1]]) # TODO: delete this print\n",
    "\n",
    "mgarch_volatilities = {}\n",
    "\n",
    "for date, cov_matrix in mgarchDict.items():\n",
    "    volatilities = np.sqrt(np.diag(cov_matrix.values))\n",
    "    mgarch_volatilities[date] = pd.DataFrame(data = volatilities, index = cov_matrix.index, columns = [\"volatility\"])\n",
    "\n",
    "# now mgarch_volatilities is a dictionary that contains the volatilities of the 3 assets for every day with the same key of the mgarchDict dictionary(the timestamp)\n",
    "    \n",
    "# now separate the volatilities of the 3 assets in 3 different dataframes\n",
    "volatility_dict_aapl_mgarch = {}\n",
    "volatility_dict_ibm_mgarch = {}\n",
    "volatility_dict_mcd_mgarch = {}\n",
    "\n",
    "for date, volatilities in mgarch_volatilities.items():\n",
    "    volatility_dict_aapl_mgarch[date] = volatilities.loc[7][\"volatility\"] # 7 is the PERMCO code of AAPL\n",
    "    volatility_dict_ibm_mgarch[date] = volatilities.loc[20990][\"volatility\"] # 20990 is the PERMCO code of IBM\n",
    "    volatility_dict_mcd_mgarch[date] = volatilities.loc[21177][\"volatility\"] # 21177 is the PERMCO code of MCD\n",
    "\n",
    "# Convert the dictionaries to DataFrames for easier manipulation and plotting\n",
    "df_volatility_aapl_mgarch = pd.DataFrame(list(volatility_dict_aapl_mgarch.items()), columns=['Date', 'AAPL Volatility'])\n",
    "df_volatility_ibm_mgarch = pd.DataFrame(list(volatility_dict_ibm_mgarch.items()), columns=['Date', 'IBM Volatility'])\n",
    "df_volatility_mcd_mgarch = pd.DataFrame(list(volatility_dict_mcd_mgarch.items()), columns=['Date', 'MCD Volatility'])\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "df_volatility_aapl_mgarch.set_index('Date', inplace=True)\n",
    "df_volatility_ibm_mgarch.set_index('Date', inplace=True)\n",
    "df_volatility_mcd_mgarch.set_index('Date', inplace=True)\n",
    "\n",
    "# Plot the real volatilities of the 3 assets\n",
    "plt.figure(figsize=(18, 11))\n",
    "plt.plot(df_volatility_aapl_mgarch, label='AAPL Volatility')\n",
    "plt.plot(df_volatility_ibm_mgarch, label='IBM Volatility')\n",
    "plt.plot(df_volatility_mcd_mgarch, label='MCD Volatility')\n",
    "plt.legend()\n",
    "plt.title(\"MGARCH Volatilities of the 3 assets\")\n",
    "plt.xlabel(\"Time(days)\")\n",
    "plt.ylabel(\"Volatility(%)\")\n",
    "\n",
    "# Adding vertical lines for specific events\n",
    "plt.axvline(pd.Timestamp('2020-02-24'), color='gray', linestyle='--', lw=2)  # COVID start\n",
    "plt.axvline(pd.Timestamp('2022-02-24'), color='orange', linestyle='--', lw=2)  # Ukraine War start\n",
    "\n",
    "# Annotations for the events\n",
    "#plt.text(pd.Timestamp('2020-02-24'), plt.ylim()[1], 'COVID', horizontalalignment='center', color='gray')\n",
    "plt.text(pd.Timestamp('2022-02-24'), plt.ylim()[1], 'Ukraine War', horizontalalignment='center', color='orange')\n",
    "\n",
    "# Set x-axis limits to the first and last index of your time series data\n",
    "plt.xlim(df_volatility_aapl_mgarch.index[0], df_volatility_aapl_mgarch.index[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE THERE IS THE VOLATILITY ANALYSIS OF THE MGARCH PREDICTOR COMPARED TO THE REAL VOLATILITY\n",
    "startDateFilter = validationDataWithPrices.index[0]\n",
    "endDateFilter = validationDataWithPrices.index[-1]\n",
    "\n",
    "plot_prices_volatilities_for_mgarch(validationDataWithPrices, df_volatility_aapl[\"AAPL Volatility\"], startDateFilter, endDateFilter, df_volatility_aapl_mgarch['AAPL Volatility'], 'AAPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Phase for MGARCH predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test phase i will use the best Mgarch model found in the validation phase to see the performance of the model on the test set and i will plot some charts to compare the volatility predicted by the model with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the date for the test phase\n",
    "startingTestDate = testDataWithPercentageChange.index[0].strftime(\"%Y-%m-%d\")\n",
    "endingTestDate = testDataWithPercentageChange.index[-1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# MGARCH precomputed in R due to computational complexity\n",
    "# now to automatize the process of obtaining the MGARCH predictions, i will call a function that will execute the r script that calculates the MGARCH predictions\n",
    "# so the csv is updated with the MGARCH predictions and then i will read the csv file to obtain the MGARCH predictions\n",
    "\n",
    "callRprocessAndUpdateCSVfile(\"AllCovMatricesForTesting.csv\", \"experiments/mgarchAnalysis/dccGarchTrainValidationTest.r\")\n",
    "\n",
    "# read from this file the covariance matrices calculated for the test phase\n",
    "mgarch_cond_cov = pd.read_csv(\"mgarch_predictors_from_R/stocks/FlattenedAllCovMatricesForTesting.csv\", index_col=None)\n",
    "\n",
    "# here we are obtaining the covariance matrix calculated for every day from the csv file;\n",
    "# so this covariance matrix estimation is done with real returns and not with interpolated returns\n",
    "Sigmas = from_row_matrix_to_covariance(mgarch_cond_cov.values, stocksPercentageChangeReturn.shape[1]) / 10000 # returns.shape[1] gives the number of columns in the returns DataFrame, which corresponds to the number of assets in the portfolio\n",
    "\n",
    "print(\"sigma shape:\", Sigmas.shape)\n",
    "# Remove the training dataset \n",
    "trainingSetLength = len(trainingDataWithPercentageChange)\n",
    "validationSetLength = len(validationDataWithPercentageChange)\n",
    "times = stocksPercentageChangeReturn.index[trainingSetLength + validationSetLength:]\n",
    "\n",
    "print(\"len of times: \", len(times))\n",
    "\n",
    "mgarchDict = {times[t]: pd.DataFrame(Sigmas[t], index=stocksPercentageChangeReturn.columns, columns=stocksPercentageChangeReturn.columns) for t in range(Sigmas.shape[0])}\n",
    "\n",
    "print(\"mgarchDict len:\", len(mgarchDict))\n",
    "# calculate the covariance matrix for the test phase\n",
    "\n",
    "# print just the first key and value of the dictionary\n",
    "print(list(mgarchDict.keys())[0]) # TODO: delete this print\n",
    "print(mgarchDict[list(mgarchDict.keys())[0]]) # TODO: delete this print\n",
    "print(\"\\n\")\n",
    "print(list(mgarchDict.keys())[-1]) # TODO: delete this print\n",
    "print(mgarchDict[list(mgarchDict.keys())[-1]]) # TODO: delete this print\n",
    "\n",
    "mgarch_volatilities = {}\n",
    "\n",
    "for date, cov_matrix in mgarchDict.items():\n",
    "    volatilities = np.sqrt(np.diag(cov_matrix.values))\n",
    "    mgarch_volatilities[date] = pd.DataFrame(data = volatilities, index = cov_matrix.index, columns = [\"volatility\"])\n",
    "\n",
    "# now mgarch_volatilities is a dictionary that contains the volatilities of the 3 assets for every day with the same key of the mgarchDict dictionary(the timestamp)\n",
    "    \n",
    "# now separate the volatilities of the 3 assets in 3 different dataframes\n",
    "volatility_dict_aapl_mgarch = {}\n",
    "volatility_dict_ibm_mgarch = {}\n",
    "volatility_dict_mcd_mgarch = {}\n",
    "\n",
    "for date, volatilities in mgarch_volatilities.items():\n",
    "    volatility_dict_aapl_mgarch[date] = volatilities.loc[7][\"volatility\"] # 7 is the PERMCO code of AAPL\n",
    "    volatility_dict_ibm_mgarch[date] = volatilities.loc[20990][\"volatility\"] # 20990 is the PERMCO code of IBM\n",
    "    volatility_dict_mcd_mgarch[date] = volatilities.loc[21177][\"volatility\"] # 21177 is the PERMCO code of MCD\n",
    "\n",
    "# Convert the dictionaries to DataFrames for easier manipulation and plotting\n",
    "df_volatility_aapl_mgarch = pd.DataFrame(list(volatility_dict_aapl_mgarch.items()), columns=['Date', 'AAPL Volatility'])\n",
    "df_volatility_ibm_mgarch = pd.DataFrame(list(volatility_dict_ibm_mgarch.items()), columns=['Date', 'IBM Volatility'])\n",
    "df_volatility_mcd_mgarch = pd.DataFrame(list(volatility_dict_mcd_mgarch.items()), columns=['Date', 'MCD Volatility'])\n",
    "\n",
    "# print the first 3 elements of the dataframe\n",
    "print(df_volatility_aapl_mgarch.head(3)) # TODO: delete this print\n",
    "\n",
    "# now print the first 3 entries of the dictionary\n",
    "print(list(volatility_dict_aapl_mgarch.items())[:3]) # TODO: delete this print\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "df_volatility_aapl_mgarch.set_index('Date', inplace=True)\n",
    "df_volatility_ibm_mgarch.set_index('Date', inplace=True)\n",
    "df_volatility_mcd_mgarch.set_index('Date', inplace=True)\n",
    "\n",
    "# Plot the real volatilities of the 3 assets\n",
    "plt.figure(figsize=(18, 11))\n",
    "plt.plot(df_volatility_aapl_mgarch, label='AAPL Volatility')\n",
    "plt.plot(df_volatility_ibm_mgarch, label='IBM Volatility')\n",
    "plt.plot(df_volatility_mcd_mgarch, label='MCD Volatility')\n",
    "plt.legend()\n",
    "plt.title(\"MGARCH Volatilities of the 3 assets\")\n",
    "plt.xlabel(\"Time(days)\")\n",
    "plt.ylabel(\"Volatility(%)\")\n",
    "\n",
    "# Adding vertical lines for specific events\n",
    "plt.axvline(pd.Timestamp('2020-02-24'), color='gray', linestyle='--', lw=2)  # COVID start\n",
    "plt.axvline(pd.Timestamp('2022-02-24'), color='orange', linestyle='--', lw=2)  # Ukraine War start\n",
    "\n",
    "# Set x-axis limits to the first and last index of your time series data\n",
    "plt.xlim(df_volatility_aapl_mgarch.index[0], df_volatility_aapl_mgarch.index[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE THERE IS THE VOLATILITY ANALYSIS OF THE MGARCH PREDICTOR COMPARED TO THE REAL VOLATILITY\n",
    "startDateFilter = testDataWithPrices.index[0]\n",
    "endDateFilter = testDataWithPrices.index[-1]\n",
    "\n",
    "plot_prices_volatilities_for_mgarch(testDataWithPrices, df_volatility_aapl[\"AAPL Volatility\"], startDateFilter, endDateFilter, df_volatility_aapl_mgarch['AAPL Volatility'], 'AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE THERE IS THE VOLATILITY ANALYSIS OF THE MGARCH PREDICTOR COMPARED TO THE REAL VOLATILITY\n",
    "plot_prices_volatilities_for_mgarch(testDataWithPrices, df_volatility_ibm[\"IBM Volatility\"], startDateFilter, endDateFilter, df_volatility_ibm_mgarch['IBM Volatility'], 'IBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE THERE IS THE VOLATILITY ANALYSIS OF THE MGARCH PREDICTOR COMPARED TO THE REAL VOLATILITY\n",
    "plot_prices_volatilities_for_mgarch(testDataWithPrices, df_volatility_mcd[\"MCD Volatility\"], startDateFilter, endDateFilter, df_volatility_mcd_mgarch['MCD Volatility'], 'MCD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW I DO THE LAST THING; THE COMPARISON BETWEEN THE MGARCH PREDICTOR AND THE PRESCIENT PREDICTOR IN TERMS OF \n",
    "# LOGLIKELIHOOD, LOGLIKELIHOOD REGRET AND MSE\n",
    "\n",
    "# first of all define the starting and ending date for the test phase: take the first and the last date of the test dataset\n",
    "\n",
    "startingTestDate = testDataWithPercentageChange.index[0].strftime(\"%Y-%m-%d\")\n",
    "endingTestDate = testDataWithPercentageChange.index[-1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "start_date = pd.to_datetime(startingTestDate, format=\"%Y-%m-%d\")\n",
    "end_date = pd.to_datetime(endingTestDate, format=\"%Y-%m-%d\")\n",
    "\n",
    "names = [\"MGARCH\", \"PRESCIENT\"]\n",
    "\n",
    "#these predictors are all dictionaries where each entry contains a Pandas DataFrame representing a covariance matrix of returns at each timestamp.  \n",
    "predictors_temp = [mgarchDict, prescientDict]\n",
    "predictors = [] # so this is a list of dictionaries\n",
    "\n",
    "for predictor in predictors_temp:\n",
    "    predictors.append({t: predictor[t] for t in predictor.keys() if t >= start_date and t <= end_date})\n",
    "\n",
    "mgarchDict = predictors[0]\n",
    "mgarchDict = {k: mgarchDict[k] for k in list(mgarchDict)[19:]}\n",
    "predictors[0] = mgarchDict\n",
    "\n",
    "# now remove the first 19 entries from the predictor 1(prescient dict)\n",
    "prescientDict = predictors[1]\n",
    "\n",
    "# now remove the first 19 entries from the prescient dict\n",
    "prescientDict = {k: prescientDict[k] for k in list(prescientDict)[19:]}\n",
    "predictors[1] = prescientDict\n",
    "\n",
    "print(\"size of the first predictor: \" + str(len(predictors[0])))\n",
    "print(\"size of the second predictor: \" + str(len(predictors[1])))\n",
    "\n",
    "\n",
    "# make an assert to check if the length of the two predictors is the same\n",
    "assert len(predictors[0]) == len(predictors[1])\n",
    "\n",
    "# make an assert to check if the timestamps of the two predictors are the same\n",
    "assert predictors[0].keys() == predictors[1].keys()\n",
    "\n",
    "# print the first timestamp of the two predictors\n",
    "print(\"first timestamp of the first predictor: \" + str(list(predictors[0].keys())[0]))\n",
    "print(\"first timestamp of the second predictor: \" + str(list(predictors[1].keys())[0]))\n",
    "\n",
    "# if we are here, it means that the two predictors have the same length and the same timestamps so i can measure the performance of the two predictors\n",
    "\n",
    "#\n",
    "# LOG-LIKELIHOODS\n",
    "#\n",
    "\n",
    "'''\n",
    "    this dictionary has a shape like this:\n",
    "    {\n",
    "        RW: pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times),\n",
    "        EWMA: pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times),\n",
    "        MGARCH: pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times),\n",
    "        PRESCIENT: pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times),\n",
    "    }\n",
    "\n",
    "    where each pd.series is a series of log-likelihoods for each timestamp: so there is the log-likelihood value for each timestamp\n",
    "'''\n",
    "\n",
    "log_likelihoods = {}\n",
    "for i, predictorDict in enumerate(predictors):\n",
    "\n",
    "    # if the predictor is the prescient predictor, i have to use the uniformly distributed dataset\n",
    "    if names[i] == \"PRESCIENT\":\n",
    "        returns_temp = uniformlyDistributedReturns.loc[pd.Series(predictorDict).index].values[1:]\n",
    "    \n",
    "    else:\n",
    "        returns_temp = stocksPercentageChangeReturn.loc[pd.Series(predictorDict).index].values[1:]\n",
    "\n",
    "    times = pd.Series(predictorDict).index[1:]\n",
    "    Sigmas_temp = np.stack([predictorDict[t].values for t in predictorDict.keys()])[:-1]       \n",
    "    log_likelihoods[names[i]] = pd.Series(log_likelihood(returns_temp, Sigmas_temp), index=times)\n",
    "\n",
    "\n",
    "# Iterate through each predictor in the log_likelihoods dictionary\n",
    "for name in log_likelihoods.keys():\n",
    "    if name == 'PRESCIENT':\n",
    "        # Resample by quarter, take the mean, and plot with specific color and label\n",
    "        log_likelihoods[name].resample(\"Q\").mean().plot(label=name, c=\"k\")\n",
    "    else:\n",
    "        # Resample by quarter, take the mean, and plot with default settings\n",
    "        log_likelihoods[name].resample(\"Q\").mean().plot(label=name)\n",
    "\n",
    "plt.xlabel('Time(quarter)')  # Set the x-axis label\n",
    "plt.ylabel('Log Likelihood')  # Set the y-axis label\n",
    "plt.title('Quarterly Mean Log Likelihood by Predictor')  # Set the title of the plot\n",
    "plt.legend()  # Show the legend to identify each predictor\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "\n",
    "'''\n",
    "    this dictionary has a shape like this:\n",
    "    {\n",
    "        RW: pd.Series(...),\n",
    "        EWMA: pd.Series(...),\n",
    "        MGARCH: pd.Series(...),\n",
    "        PRESCIENT: pd.Series(...),\n",
    "    }\n",
    "\n",
    "    where each pd.series is a series of regret for each timestamp: so there is the \n",
    "    regret value (the difference between the log-likelihood of the prescient model and the log-likelihood of the model) for each timestamp\n",
    "'''\n",
    "regrets = {}\n",
    "for name in log_likelihoods:\n",
    "    regrets[name] =  log_likelihoods[\"PRESCIENT\"] - log_likelihoods[name]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "for name in names:\n",
    "    if name == 'PRESCIENT':\n",
    "        pass\n",
    "    else:\n",
    "        regrets[name].resample(\"Q\").mean().plot(label=name)\n",
    "plt.legend(bbox_to_anchor=(1, 1.1), loc='center', ncols=4, labels=names[:-1], scatterpoints=1, markerscale=5);\n",
    "plt.xlabel(\"Time(quarter)\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(\"Regret\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for name in regrets:\n",
    "    if name != \"PRESCIENT\":\n",
    "\n",
    "        #Each data point in the regret series now represents the average regret for a respective quarter. If the original series spans multiple years, then the number of data points in regret will be the number of quarters in that time frame.\n",
    "        regret = regrets[name].resample(\"Q\").mean() #it resamples the regret Series to a quarterly frequency, This gives the average regret for each quarter rather than daily regret values  \n",
    "        # so the regret variable is a series of average regret for each quarter\n",
    "        \n",
    "        regretMetrics = (np.mean(regret).round(1), np.std(regret).round(1), np.max(regret).round(1))\n",
    "        # the round(1) function to each of these metrics, which rounds the result to one decimal place,\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"meanRegret: {regretMetrics[0]:.3f}\")\n",
    "print(f\"stdRegret: {regretMetrics[1]:.3f}\")\n",
    "print(f\"maxRegret: {regretMetrics[2]:.3f}\")\n",
    "\n",
    "# copy the log-likelihoods dictionary\n",
    "log_likelihoods_copy = log_likelihoods.copy()\n",
    "\n",
    "# do the same thing for log-likelihoods dictionary\n",
    "for name in log_likelihoods_copy:\n",
    "    logLikelihood = log_likelihoods_copy[name].resample(\"Q\").mean()\n",
    "    logLikelihoodMetrics = (np.mean(logLikelihood).round(1), np.std(logLikelihood).round(1), np.max(logLikelihood).round(1))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"meanLoglikelihood{name}: {logLikelihoodMetrics[0]:.3f}\")\n",
    "    print(f\"stdLoglikelihood{name}: {logLikelihoodMetrics[1]:.3f}\")\n",
    "    print(f\"maxLoglikelihood{name}: {logLikelihoodMetrics[2]:.3f}\")\n",
    "\n",
    "\n",
    "#\n",
    "# RMSEs\n",
    "#\n",
    "\n",
    "print(\"lenght of prescientDict: \", len(prescientDict))\n",
    "print(\"lenght of mgarchDict: \", len(mgarchDict))\n",
    "        \n",
    "for i, predictorDict in enumerate(predictors):\n",
    "    if names[i] != \"PRESCIENT\":\n",
    "        print(\"lenght of predictorDict: \", len(predictorDict))\n",
    "        RMSEs = RMSE(testDataWithPercentageChange, predictorDict, prescientDict, start_date)\n",
    "        print(\"\\n\" + names[i] + \" RMSE\")\n",
    "\n",
    "        # Calculate mean, standard deviation, and max value of the RMSEs\n",
    "        mean_rmse = np.mean(list(RMSEs.values()))\n",
    "        std_rmse = np.std(list(RMSEs.values()))\n",
    "        max_rmse = np.max(list(RMSEs.values()))\n",
    "\n",
    "        print(f\"mean: {mean_rmse:.10f}\")\n",
    "        print(f\"std: {std_rmse:.10f}\")\n",
    "        print(f\"max: {max_rmse:.10f}\")\n",
    "\n",
    "\n",
    "print(\"lenght of rmses: \", len(RMSEs))\n",
    "print(\"values of rmses: \", RMSEs)\n",
    "\n",
    "# Convert Timestamps to strings for plotting\n",
    "timestamps = [ts.strftime('%Y-%m-%d') for ts in RMSEs.keys()]\n",
    "rmse_values = list(RMSEs.values())\n",
    "\n",
    "# Plot the RMSEs with improved formatting\n",
    "plt.figure(figsize=(14, 7))  # Increase the figure size for better readability\n",
    "plt.plot(timestamps, rmse_values, marker='o', linestyle='-', label='MGARCH', color='b')\n",
    "\n",
    "# Set the x-axis to only include the dates from the dictionary\n",
    "plt.xticks(timestamps, rotation=45)\n",
    "\n",
    "# Remove the left margin\n",
    "plt.margins(x=0)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Time (Quarter)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSEs')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "# RMSEs for single assets\n",
    "#\n",
    "        \n",
    "# take the first timestamp contained in prescientDict and use it as the start date\n",
    "startDAte = list(prescientDict.keys())[0]\n",
    "\n",
    "# filter every volatility dictionary to get only the values that are greater than or equal to the start date\n",
    "volatility_dict_aapl_filtered = {k: v for k, v in volatility_dict_aapl.items() if k >= startDAte}\n",
    "volatility_dict_ibm_filtered = {k: v for k, v in volatility_dict_ibm.items() if k >= startDAte}\n",
    "volatility_dict_mcd_filtered = {k: v for k, v in volatility_dict_mcd.items() if k >= startDAte}\n",
    "\n",
    "volatility_dict_aapl_mgarch_filtered = {k: v for k, v in volatility_dict_aapl_mgarch.items() if k >= startDAte}\n",
    "volatility_dict_ibm_mgarch_filtered = {k: v for k, v in volatility_dict_ibm_mgarch.items() if k >= startDAte}\n",
    "volatility_dict_mcd_mgarch_filtered = {k: v for k, v in volatility_dict_mcd_mgarch.items() if k >= startDAte}\n",
    "\n",
    "\n",
    "print(\"lenght of volatility_dict_aapl: \", len(volatility_dict_aapl_filtered))\n",
    "print(\"lenght of volatility_dict_aaapl_mgarch: \", len(volatility_dict_aapl_mgarch_filtered))\n",
    "# get the rmse of single assets. i take just aapl, ibm and mcd\n",
    "        \n",
    "RMSEs_aapl_dict = RMSEforSingleVolatility(testDataWithPercentageChange, volatility_dict_aapl_filtered, volatility_dict_aapl_mgarch_filtered, start_date)\n",
    "RMSEs_ibm_dict = RMSEforSingleVolatility(testDataWithPercentageChange, volatility_dict_ibm_filtered, volatility_dict_ibm_mgarch_filtered, start_date)\n",
    "RMSEs_mcd_dict = RMSEforSingleVolatility(testDataWithPercentageChange, volatility_dict_mcd_filtered, volatility_dict_mcd_mgarch_filtered, start_date)\n",
    "\n",
    "print(\"lenght of RMSEs_aapl: \", len(RMSEs_aapl_dict))\n",
    "print(\"values of RMSEs_aapl: \", RMSEs_aapl_dict)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"RMSEs for AAPL\")\n",
    "\n",
    "# Calculate mean, standard deviation, and max value of the RMSEs. the RMSEs are dictionaries whose key is the timestamp and the value is the rmse value\n",
    "mean_rmse_aapl = np.mean(list(RMSEs_aapl_dict.values()))\n",
    "std_rmse_aapl = np.std(list(RMSEs_aapl_dict.values()))\n",
    "max_rmse_aapl = np.max(list(RMSEs_aapl_dict.values()))\n",
    "\n",
    "print(f\"mean: {mean_rmse_aapl:.10f}\")\n",
    "print(f\"std: {std_rmse_aapl:.10f}\")\n",
    "print(f\"max: {max_rmse_aapl:.10f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"RMSEs for IBM\")\n",
    "\n",
    "# Calculate mean, standard deviation, and max value of the RMSEs. the RMSEs are dictionaries whose key is the timestamp and the value is the rmse value\n",
    "mean_rmse_ibm = np.mean(list(RMSEs_ibm_dict.values()))\n",
    "std_rmse_ibm = np.std(list(RMSEs_ibm_dict.values()))\n",
    "max_rmse_ibm = np.max(list(RMSEs_ibm_dict.values()))\n",
    "\n",
    "print(f\"mean: {mean_rmse_ibm:.10f}\")\n",
    "print(f\"std: {std_rmse_ibm:.10f}\")\n",
    "print(f\"max: {max_rmse_ibm:.10f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"RMSEs for MCD\")\n",
    "\n",
    "# Calculate mean, standard deviation, and max value of the RMSEs. the RMSEs are dictionaries whose key is the timestamp and the value is the rmse value\n",
    "mean_rmse_mcd = np.mean(list(RMSEs_mcd_dict.values()))\n",
    "std_rmse_mcd = np.std(list(RMSEs_mcd_dict.values()))\n",
    "max_rmse_mcd = np.max(list(RMSEs_mcd_dict.values()))\n",
    "\n",
    "print(f\"mean: {mean_rmse_mcd:.10f}\")\n",
    "print(f\"std: {std_rmse_mcd:.10f}\")\n",
    "print(f\"max: {max_rmse_mcd:.10f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
